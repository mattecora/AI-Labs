\documentclass[letterpaper,headings=standardclasses]{scrartcl}

\usepackage[margin=1in,includefoot]{geometry}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsmath}

\title{Homework 2}
\subtitle{CS 411 - Artificial Intelligence I - Fall 2019}
\author{Matteo Corain 650088272}

\begin{document}

\maketitle

\section{Question 1}

\subsection{Goal-based agent}

A \emph{goal-based agent} is a type of agent for which the decision to take in each situation, in terms of actions to be performed, cannot be described by a static set of rules, like in a \emph{reflex agent} (either \emph{simple} or \emph{model-based}). Goal-based agents are \emph{deliberative}, in the sense that they go through some sort of thinking process to figure out what action to take; in fact, they are able to predict the expected outcome of the different actions, which is then used to identify the one to be performed. The goal of an agent is represented by a \emph{goal function} $g : \mathcal{S} \rightarrow \{0;1\}$, defined by the designer, which associates to each possible state $s$ of the environment a binary value according to the following logic:

$$ g(s) = \begin{cases} 1 & \text{if state } s \text{ fulfills the goal} \\ 0 & \text{otherwise}\end{cases} $$

In order to achieve their objective, especially in case this is non-trivial and requires more than a single action to be fulfilled, goal-based agents cannot focus only on the present, but need to introduce a form of \emph{planning} of the future. An essential precondition to planning is that the environment the agent works in is somehow sequential, i.e. the actions the agent has taken in the past somehow affect the reward it is going to receive for the subsequent ones.

During the planning phase, which happens offline (no action is performed in the meanwhile), the agent explores a planning space in order to identify a path (that is, a sequence of actions), possibly the shortest, that is expected to make the environment transition to a state in which the goal function returns a positive value. This operation is generally not trivial, given that the search space grows exponentially with the number of actions $|\mathcal{A}|$.

Once a path capable of satisfying the goal has been identified, the agent starts acting according to the identified sequence. If the environment is deterministic, then it is guaranteed that, after all actions in the path have been performed, its state will be such that the goal function returns 1; this means that the planning phase may be executed only once when the agent is first started. If the environment is stochastic, instead, the planning phase should be (partially) repeated after each action: in fact, in this case there is no guarantee that the environment has transitioned to the expected state and, if not, the agent must react accordingly, possibly recomputing the path inside the search space.

\subsection{Utility-based agent}

A goal-based agent acts in a way that, sooner or later, its goal will finally be achieved; in case there are many alternative ways for fulfilling it, this may in general lead to sub-optimal behaviors, since there is no concept of ranking of the different solutions (the goal function always returns 1 if the goal is fulfilled, in whatever way). A \emph{utility-based agent} tackles with this problem by associating a score, called \emph{utility}, to each alternative solution it has been able to compute that achieves the agent's goal.

In order to associate paths in the search space to their utility score, utility-based agents make use of a so-called \emph{utility function} $u : \mathcal{S} \rightarrow \mathbb{R}$, which is the equivalent of the goal function for a goal-based agent, the difference being that the utility function associates a real number to each state. This function generally represents a sort of internal estimation (based on the limited knowledge of the agent) of the performance measure.

A utility-based agent behaves similarly to a goal-based agent, in the sense that it also has to perform a planning phase in order to compute possible paths in the search space. The stopping condition, however, is different: in this case, in fact, the utility function assigns to each path a real number. The agent will therefore select the sequence of actions that, within a certain future time window, yields the best expected utility value. Also in this case, if the environment is stochastic, the expected utility as computed by the agent should always be compared to the actual reward obtained by performing each action, possibly recomputing the path to be followed.

Utility-based agents are more flexible than goal-based agent, especially when they are required to fulfill conflicting goals. In fact, while a goal-based agent may simply decide to target a single goal (possibly the one achievable in the lower number of actions), an utility-based agent aims to find the solution able to yield the overall best expected reward.

\section{Question 2}

\subsection{Fully/partially observable}

An environment is classified as fully observable if the agent is equipped with sensors capable to perceive at any point in time all the aspects of the environment that are relevant for the selection of the action to be performed; otherwise, the environment is classified as partially observable.

In this case, the variables of interest for the thorough description of the state of the environment are represented by:

\begin{itemize}

\item The \emph{location} of the robot;
\item The \emph{presence of a star} in each location the robot can pass through.

\end{itemize}

Supposing to associate each location with a natural number $n \in [1; 12]$, the state $S$ of the environment is described in formal terms by a set of $n$-dimensional vectors, with $n = 13$, in which:

\begin{itemize}

\item The first component, $s_1 \in [1;12]$, represents the square the robot is currently in;
\item All the following components, $s_i \in \{\text{Star}; \text{No-Star}\}, i = 2 ... 13$, represent the presence or absence of a star in the $(i-1)$-th square.

\end{itemize}

If the robot is equipped with a set of sensors capable of perceiving all the thirteen components that describe the environment's state at once, then the environment is \emph{fully observable}. If we consider instead a much simpler robot, capable for example of sensing its location and the presence of a star only within a more limited number of squares from its position, then the environment has to be considered \emph{partially observable}.

\subsection{Deterministic/stochastic}

An environment is classified as deterministic if it is possible to precisely state what the next state of the environment will be, given the current state and the action performed by the agent; if this is not possible, then the environment is classified as stochastic.

In this case, conceptually speaking, once that the state of the environment and the action the robot is going to perform are known, it is possible to infer what the next state of the environment will be. In other words, from a theoretical point of view, this environment does not present any source of uncertainty: it is very clear what the next state will be when the agent will perform its actions; thus, this environment has to be considered \emph{deterministic}.

However, if we suppose to implement the described system into a real-world object, then a lot of different sources of uncertainty would be introduced: for example, the robot may find unexpected obstacles on its way or may broke itself, preventing it from modifying the state of the environment in the expected way. Therefore, a physical implementation of the described environment has to be considered \emph{stochastic}.

\subsection{Dynamic/static}

An environment is classified as dynamic if it can change its state without the direct intervention of the agent; otherwise, the environment is classified as static.

In this case, there are many ways in which the environment could change without the intervention of the agent; for example, stars may be added or removed at any time, or maybe the lights may turn off at regular intervals preventing the agent to perceive their location. If we consider those factors in the description of the environment, then it has to be considered \emph{dynamic}; if instead we suppose that the environment for the robot is given and no external factor can act on it, then it has to be considered \emph{static}.

\subsection{Episodic/sequential}

An environment is classified as episodic when the actions that the agent performed in the past do not affect in any way its decision for the following one; if instead the sequence of past actions influences the decision of the agent, then the environment is classified as sequential.

The described case is a typically \emph{sequential} scenario: the ability of the robot to move in the environment, in fact, impacts on its future decisions because it has an effect on its location. In order to pick up the stars, the robot has to move in a way that allows it to reach the location in which the desired object is present; if the robot behaves correctly, this change of location will make it closer to the star and drive the following decision, so that in the end it will be able to acheive its goal through a precise sequence of actions.

\subsection{Known/unknown}

An environment is classified as known when the agent knows the rules according to which this environment works, which can be used to predict the outcome of the different actions it can take; if the agent (at least initially) lacks this knowledge, then the environment is classified as unknown.

In this case, we can suppose that the described environment works according to the following set of rules:

\begin{itemize}

\item The robot can only move inside a 3x4 matrix of locations;
\item If the robot finds itself in a spot where a star is present, then it can pick it up.

\end{itemize}

If the robot knows this boundaries to the actions it can choose from the beginning, for example because it was programmed in a way that enforces the fulfillment of those rules, then the described environment has to be considered \emph{known}. Instead, if the robot is initially unaware of the presence of these rules, which will be probably discovered during its operation through a learning process, then the described environment has to be considered initially \emph{unknown}.

\subsection{Continuous/discrete}

An environment is classified as continuous if the state of the environment, the time, the percepts and the actions of the agent are described by continuous variables; otherwise, the environment is classified as discrete.

In this case, we have the following:

\begin{itemize}

\item The state of the environment is described by a discrete variable, represented by a 13-dimensional vector as mentioned before;
\item The time may be handled both in a discrete way (e.g. the robot perceives and acts only at defined instants of time) or in a continuous way (e.g. the robot continuously perceives the environment and acts accordingly);
\item The percepts of the agent are described again by a discrete variable, represented by a vector whose components depend on the sensory capabilities of the robot;
\item The actions of the agent may be described both in a discrete way (if we suppose that all actions are atomic, either performed completely or not performed, and distances are measured in multiples of the dimension of a square) or in a continuous way (if we drop the previous hypothesis, e.g. the robot may perform an action only partially or move for a distance that is not multiple of the dimension of a square).

\end{itemize}

If we consider both time and actions to be handled in a discrete way, then the described environment has to be classified as \emph{discrete} (it is not described by any continuous variable); otherwise, it has to be classified as \emph{continuous} (at least a continuous variable is necessary for its description).

\end{document}